{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### DNN for Hand Recognition\n\nTODO: \n1. Optimize hyperparams via (https://www.tensorflow.org/tutorials/keras/keras_tuner) \n2. Filter outliers in training data \n3. Data Augmentation\n4. NN Pruning\n5. Speed up data generation","metadata":{}},{"cell_type":"markdown","source":"# Imports and Training Params","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tqdm.notebook import tqdm\nfrom multiprocessing import Pool\nimport matplotlib.pyplot as plt\nimport json\nimport zipfile\nimport os\nimport gc\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nBASE_TRAIN_PATH = \"/kaggle/input/asl-signs/\"\nTRAIN_FILE = \"/kaggle/input/asl-signs/train.csv\"\nINDEX_MAP_FILE = '/kaggle/input/asl-signs/sign_to_prediction_index_map.json'\nMODEL_OUT_PATH = '/kaggle/working/modelie'\n\ndata_columns = ['x', 'y', 'z']\n\nuse_generator = False \nfind_optimal_params = True\nfilter_outliers = True \ntraining = True\nlocal_inference_test = False\n\nepochs = 50\nbatch_size = 100\n\nrows_per_frame = 543 #Number of landmarks per frame \nnum_classes = 250 \ndropout_rate = .3\n\nmax_length = 30 # length that input is padded/truncated to \n\nwarnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n\nLEFT_HAND_OFFSET = 468\nPOSE_OFFSET = LEFT_HAND_OFFSET+21\nRIGHT_HAND_OFFSET = POSE_OFFSET+33\nROWS_PER_FRAME = 543\nlip_landmarks = [61, 185, 40, 39, 37,  0, 267, 269, 270, 409,\n                 291,146, 91,181, 84, 17, 314, 405, 321, 375, \n                 78, 191, 80, 81, 82, 13, 312, 311, 310, 415, \n                 95, 88, 178, 87, 14,317, 402, 318, 324, 308]\n\nleft_hand_landmarks = list(range(LEFT_HAND_OFFSET, LEFT_HAND_OFFSET+21))\nright_hand_landmarks = list(range(RIGHT_HAND_OFFSET, RIGHT_HAND_OFFSET+21))\n\npoint_landmarks = [item for sublist in [lip_landmarks, left_hand_landmarks, right_hand_landmarks] for item in sublist]\n\ntrain = pd.read_csv(TRAIN_FILE)\nwith open(INDEX_MAP_FILE, 'r') as f: \n    index_map = json.load(f)\n\ntrain['label'] = train['sign'].map(lambda x: index_map[x])","metadata":{"execution":{"iopub.status.busy":"2023-03-30T12:52:18.929752Z","iopub.execute_input":"2023-03-30T12:52:18.930361Z","iopub.status.idle":"2023-03-30T12:52:19.324007Z","shell.execute_reply.started":"2023-03-30T12:52:18.930310Z","shell.execute_reply":"2023-03-30T12:52:19.322668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multiprocessed Data Loading (to Disk)","metadata":{}},{"cell_type":"code","source":"if not use_generator: \n    train_paths = [train.iloc[i].path for i in range(len(train))]\n    y = [train.iloc[i].label for i in range(len(train))]\n\n    def fill_x(path): \n        partial_x = []\n        data = pd.read_parquet(os.path.join(BASE_TRAIN_PATH, path), columns=data_columns)\n\n        data.fillna(0, inplace=True)\n\n        n = int(len(data)/rows_per_frame)\n\n        data = data.values.reshape(n, rows_per_frame, len(data_columns)).astype(np.float32)\n        data = data[:, point_landmarks, :] # filter for the relevant pose landmarks \n        \n        data = np.nan_to_num(data) # Should not have any Nans here \n        data = np.reshape(data, (n, len(point_landmarks)*3), order = 'C').astype(np.float32)\n        return data\n\n    with Pool(processes=10) as pool: \n        x_ragged = list(tqdm(pool.imap(fill_x, train_paths, chunksize=1000), total = 100))\n        pool.close()\n\n    x_ragged = np.array(x_ragged)\n    y = np.array(y)\n\n    x = tf.keras.utils.pad_sequences(x_ragged, padding=\"post\", truncating=\"post\", maxlen = max_length, dtype=np.float32)\n\n    del x_ragged \n    gc.collect()\n    \n    X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=1) \n\n    del x , y\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T17:11:56.813443Z","iopub.execute_input":"2023-03-28T17:11:56.813808Z","iopub.status.idle":"2023-03-28T17:11:59.667639Z","shell.execute_reply.started":"2023-03-28T17:11:56.813776Z","shell.execute_reply":"2023-03-28T17:11:59.666096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Generator\nAllows us to load more data than can fit in memory","metadata":{}},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence): \n    def __init__(self, train, list_IDs, point_landmarks, batch_size=32, max_length=30, rows_per_frame=543, \n                 data_columns=['x','y','z'], shuffle=True):\n        self.train = train\n        self.list_IDs = list_IDs\n        self.batch_size = batch_size\n        self.max_length = max_length \n        self.rows_per_frame = rows_per_frame\n        self.point_landmarks = point_landmarks\n        self.data_columns = data_columns \n        self.shuffle = shuffle\n        self.on_epoch_end()\n        \n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n    \n    def __data_generation(self, list_IDs_temp): \n        X = np.empty((self.batch_size, self.max_length, len(self.point_landmarks)*len(self.data_columns)))\n        y = np.empty((self.batch_size), dtype=int)\n        \n        for i, ID in enumerate(list_IDs_temp):\n            data = pd.read_parquet(os.path.join(BASE_TRAIN_PATH, self.train.iloc[ID].path), columns=self.data_columns)\n          \n            data.fillna(0, inplace=True)\n            n = int(len(data)/self.rows_per_frame)\n            data = data.values.reshape(n, self.rows_per_frame, len(self.data_columns)).astype(np.float32)\n            data = data[:, self.point_landmarks, :]\n            data = np.reshape(data, (n, len(self.point_landmarks)*len(self.data_columns)), order = 'C').astype(np.float32)\n            \n            if data.shape[0] > self.max_length:  \n                data = data[:self.max_length, :]\n            elif data.shape[0] < self.max_length: \n                data = np.pad(data, ((0,self.max_length - data.shape[0]),(0,0)))\n                \n            X[i,] = data\n            y[i] = self.train.iloc[ID].label\n            \n        return X, y\n    \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n    \n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        # Generate data\n        X, y = self.__data_generation(indexes)\n        return X, y\n\nif use_generator:     \n    ds_params = {\n        'max_length': max_length, \n        'batch_size': batch_size, \n        'rows_per_frame': rows_per_frame,\n        'data_columns': data_columns, \n        'shuffle': True}\n\n    partition = {'train': [i for i in range(int(len(train)*.8))], 'validation': [j for j in range(int(len(train)*.8), len(train))]}\n\n    training_generator = DataGenerator(train, partition['train'], point_landmarks, **ds_params)\n    validation_generator = DataGenerator(train, partition['validation'], point_landmarks, **ds_params)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T12:57:49.178328Z","iopub.execute_input":"2023-03-30T12:57:49.178791Z","iopub.status.idle":"2023-03-30T12:57:49.211087Z","shell.execute_reply.started":"2023-03-30T12:57:49.178754Z","shell.execute_reply":"2023-03-30T12:57:49.209751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Frames Stats\n\nMean: 37.935\n\nMedian: 22\n\nStdDev: 44.177\n\nMax: 537\n\nMin: 2","metadata":{}},{"cell_type":"code","source":"def get_model(): \n    model = tf.keras.Sequential([\n        tf.keras.Input(shape=(max_length, len(point_landmarks)*3), dtype=np.float32), \n        tf.keras.layers.Masking(mask_value=0, input_shape=(max_length, len(point_landmarks)*3)),\n        tf.keras.layers.Dense(units=256, activation='relu'), # Generally want hidden layers to be between the size of the input and output layers \n        tf.keras.layers.Dropout(dropout_rate),\n        tf.keras.layers.LayerNormalization(), \n        tf.keras.layers.Dense(units=256, activation='relu'),\n        tf.keras.layers.Dropout(dropout_rate),\n        tf.keras.layers.LSTM(256),\n        tf.keras.layers.Dropout(dropout_rate),\n        tf.keras.layers.LayerNormalization(),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(units = num_classes, activation='softmax', name='outie'), # Output size is <256>\n    ])\n    model.compile(optimizer = \"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n    return model \n\nmodel = get_model() ","metadata":{"execution":{"iopub.status.busy":"2023-03-28T17:57:53.732615Z","iopub.execute_input":"2023-03-28T17:57:53.732983Z","iopub.status.idle":"2023-03-28T17:57:55.214272Z","shell.execute_reply.started":"2023-03-28T17:57:53.732952Z","shell.execute_reply":"2023-03-28T17:57:55.213189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optimal Hyper Parameter Search","metadata":{}},{"cell_type":"code","source":"#Hyper Param tuning \n!pip install -q -U keras-tuner\nimport keras_tuner as kt \n\ndef get_tuned_model(hp): \n    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n    hp_units_2 = hp.Int('units_2', min_value=32, max_value=512, step=32)\n    hp_units_3 = hp.Int('units_3', min_value=32, max_value=512, step=32)\n    model = tf.keras.Sequential([\n        tf.keras.Input(shape=(max_length, len(point_landmarks)*3), dtype=np.float32), \n        tf.keras.layers.Masking(mask_value=0, input_shape=(max_length, len(point_landmarks)*3)),\n        tf.keras.layers.Dense(units=hp_units, activation='relu'), # Generally want hidden layers to be between the size of the input and output layers \n        tf.keras.layers.Dropout(dropout_rate),\n        tf.keras.layers.LayerNormalization(), \n        tf.keras.layers.Dense(units=hp_units_2, activation='relu'),\n        tf.keras.layers.Dropout(dropout_rate),\n        tf.keras.layers.LSTM(hp_units_3),\n        tf.keras.layers.Dropout(dropout_rate),\n        tf.keras.layers.LayerNormalization(),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(units = num_classes, activation='softmax', name='outie'), # Output size is <256>\n    ])\n    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=hp_learning_rate), \n                  loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n    return model \n\nif find_optimial_params: \n    tuner = kt.Hyperband(get_tuned_model,\n                         objective='val_accuracy',\n                         max_epochs=10,\n                         factor=3,\n                         directory='kaggle/working',\n                         project_name='intro_to_kt')\n\n    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\n    tuner.search(X_train, Y_train, validation_data=(X_val, Y_val), epochs=50, callbacks=[stop_early])\n\n    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n    print(best_hps.get('units'))\n    print(best_hps.get('units2'))\n    print(best_hps.get('units3'))\n    print(best_hps.get('learning_rate'))\n    model = tuner.hypermodel.build(best_hps)\n\n    history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=50)\n    val_acc_per_epoch = history.history['val_accuracy']\n    best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n    print('Best epoch: %d' % (best_epoch,))\n    epochs = best_epoch","metadata":{"execution":{"iopub.status.busy":"2023-03-30T13:11:41.976365Z","iopub.execute_input":"2023-03-30T13:11:41.976920Z","iopub.status.idle":"2023-03-30T13:22:02.815689Z","shell.execute_reply.started":"2023-03-30T13:11:41.976859Z","shell.execute_reply":"2023-03-30T13:22:02.812833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model fitting using either Data Generator or Numpy Arrays","metadata":{}},{"cell_type":"code","source":"callbacks = [\n        EarlyStopping(\n                monitor = \"val_accuracy\",\n                min_delta = 0, # minimium amount of change to count as an improvement\n                patience = 5, # how many epochs to wait before stopping\n                restore_best_weights=True),\n    \n        ReduceLROnPlateau(monitor = \"val_accuracy\",\n            factor = 0.5,\n            patience = 5)\n            ]\n\nif use_generator: \n    history = model.fit(x=training_generator, \n                        validation_data=validation_generator, \n                        epochs=epochs,\n                        callbacks=callbacks, \n                        use_multiprocessing=True, \n                        workers=6)\n\n    loss, acc = model.evaluate(validation_generator, verbose=2)\nelse: \n    history = model.fit(X_train, Y_train, \n                  batch_size=batch_size, \n                  epochs=epochs,\n                  validation_data = (X_val, Y_val), \n                  callbacks=callbacks)\n\n    loss, acc = model.evaluate(X_val, Y_val, verbose=2)\nprint(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))","metadata":{"execution":{"iopub.status.busy":"2023-03-28T17:58:18.405535Z","iopub.execute_input":"2023-03-28T17:58:18.406142Z","iopub.status.idle":"2023-03-28T18:00:07.281496Z","shell.execute_reply.started":"2023-03-28T17:58:18.406105Z","shell.execute_reply":"2023-03-28T18:00:07.279717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Summary\n## Loss and Accuracy Graphs","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T14:04:17.317365Z","iopub.execute_input":"2023-03-28T14:04:17.317742Z","iopub.status.idle":"2023-03-28T14:04:17.355137Z","shell.execute_reply.started":"2023-03-28T14:04:17.317711Z","shell.execute_reply":"2023-03-28T14:04:17.354340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T14:04:20.404745Z","iopub.execute_input":"2023-03-28T14:04:20.405117Z","iopub.status.idle":"2023-03-28T14:04:20.667218Z","shell.execute_reply.started":"2023-03-28T14:04:20.405084Z","shell.execute_reply":"2023-03-28T14:04:20.666155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T14:04:31.111375Z","iopub.execute_input":"2023-03-28T14:04:31.112303Z","iopub.status.idle":"2023-03-28T14:04:31.342286Z","shell.execute_reply.started":"2023-03-28T14:04:31.112244Z","shell.execute_reply":"2023-03-28T14:04:31.341144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference Model","metadata":{}},{"cell_type":"code","source":"class FeatureGenTF(tf.keras.layers.Layer):\n    def __init__(self):\n        super().__init__()\n\n    def call(self, x):\n        x = tf.gather(x, point_landmarks, axis=1)\n        x = tf.image.resize_with_pad(x, max_length, len(point_landmarks))\n        x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n        x = tf.reshape(x, (max_length, len(point_landmarks)*3))\n        x = tf.expand_dims(x,0) # THIS IS SAYING BATCH SIZE ONE TO MODEL YEAAAA\n\n        return x\n    \npreprocessing = FeatureGenTF()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T14:04:54.265008Z","iopub.execute_input":"2023-03-28T14:04:54.265404Z","iopub.status.idle":"2023-03-28T14:04:54.278834Z","shell.execute_reply.started":"2023-03-28T14:04:54.265365Z","shell.execute_reply":"2023-03-28T14:04:54.277719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_inference_model(model):\n    inputs = tf.keras.Input((543, 3), dtype=tf.float32, name=\"inputs\") \n    x = preprocessing(inputs)\n    x = model(x)\n    output = tf.keras.layers.Activation(activation=\"linear\", name=\"outputs\")(x)\n    inference_model = tf.keras.Model(inputs=inputs, outputs=output) \n    inference_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n    return inference_model\n\ninference_model = get_inference_model(model)\ninference_model.summary(expand_nested=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T14:04:57.316747Z","iopub.execute_input":"2023-03-28T14:04:57.317654Z","iopub.status.idle":"2023-03-28T14:04:58.210355Z","shell.execute_reply.started":"2023-03-28T14:04:57.317602Z","shell.execute_reply":"2023-03-28T14:04:58.209508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"converter = tf.lite.TFLiteConverter.from_keras_model(inference_model)\ntflite_model = converter.convert()\n\nwith open(\"model.tflite\", 'wb') as f:\n    f.write(tflite_model)\n\nprint(f\"Model Size: {os.stat('model.tflite').st_size}\")\n\n!zip submission.zip $model_path","metadata":{"execution":{"iopub.status.busy":"2023-03-28T14:05:09.141549Z","iopub.execute_input":"2023-03-28T14:05:09.142153Z","iopub.status.idle":"2023-03-28T14:05:28.150674Z","shell.execute_reply.started":"2023-03-28T14:05:09.142117Z","shell.execute_reply":"2023-03-28T14:05:28.149330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing inference model","metadata":{}},{"cell_type":"code","source":"!pip install tflite_runtime\nimport tflite_runtime.interpreter as tflite\ndef load_relevant_data_subset(pq_path):\n    data_columns = ['x', 'y', 'z']\n    data = pd.read_parquet(pq_path, columns=data_columns)\n    n_frames = int(len(data) / ROWS_PER_FRAME)\n    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n    return data.astype(np.float32)\n\nif local_inference_test: \n    #TODO Find out what signs the model is bad at inferring\n    interpreter = tflite.Interpreter(model_path)\n    found_signatures = list(interpreter.get_signature_list().keys())\n    prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n    p2s_map = {v: k for k, v in index_map.items()}\n    decoder = lambda x: p2s_map.get(x)\n    score = 0 \n    for i in range(int(len(train)/10)): \n        frames = load_relevant_data_subset(os.path.join(BASE_TRAIN_PATH, train.iloc[i].path))\n        output = prediction_fn(inputs=frames)\n        sign = np.argmax(output[\"outputs\"])\n        if decoder(train.iloc[i].label) == decoder(sign): \n            score += 1 \n    print((len(train)/10)/score)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T14:07:13.755066Z","iopub.execute_input":"2023-03-28T14:07:13.756096Z","iopub.status.idle":"2023-03-28T14:08:54.095819Z","shell.execute_reply.started":"2023-03-28T14:07:13.756055Z","shell.execute_reply":"2023-03-28T14:08:54.094190Z"},"trusted":true},"execution_count":null,"outputs":[]}]}